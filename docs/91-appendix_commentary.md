# Body-blindness in language studies {#appendix1}



<!-- NB: You can add comments using these tags -->

\initial{I}n this commentary, we argue that the *literate glasses* bias highlighted by Kolinsky and Morais reflects a more general bias in language and cognitive sciences, which has to do with the way language is typically studied. Indeed, researchers tend to study language in a modular way, independent of other cognitive functions and its interactions with body and environment. Here, we focus on the limitations of this approach with respect to the literacy bias and we show how theories of embodied cognition and neural re-use shed new light on these debates.^[ Cette annexe est une version adaptée du manuscrit au format de la thèse. Pour trouver la version publiée : Grasso, C., & Montant, M. (2018). Body-blindness in language studies. *L'Annee psychologique, 118*(4), 383-388.]

In their target article, @kolinsky_worries_2018 focus on a bias in the way language is studied, the literate glasses bias, which reflects the tendency of researchers to underestimate or ignore the impact of literacy on language and cognition. @kolinsky_worries_2018 highlight several consequences of wearing literate glasses on our understanding of language. In this commentary, we suggest that the literate glasses bias reflects an even bigger problem, which is the hyper-specialization of psychology and linguistics and its blindness to the (acting and perceiving) body.

In fact, language has often been studied from a unimodal and quasi-modular perspective [see also @ziegler_eyes_2018, same issue] that ignores the articulation of language processes with other cognitive processes and underestimates the role of the physical and affective body, brain, and environment in the way language is implemented. We believe that this bias is the consequence of the dominant view in cognitive psychology, according to which cognitive functions operate in independent modules rooted in specialized amodal and non-overlapping brain networks [e.g., @amalric_cortical_2018; @fodor_precis_1985; @mundale_concepts_2002; @uttal_new_2001]. From this point of view, it makes sense to study language in isolation. However, an increasing amount of data suggests that matters are much more complex.

Indeed, contrary to amodal symbolic systems of semantic and conceptual knowledge [e.g., @fodor_precis_1985; @jackendoff_english_2002], embodied theories suggest that cognition is grounded in the body and strongly influenced by our sensorimotor and emotional experiences [e.g.,@barsalou_perceptual_1999; @barsalou_grounded_2010]. Because of this, the architecture of the human brain reflects a high level of crossmodal integration, as can be seen in language development, which strongly relies on the integration of different motor and sensory modalities [@heinrich_crossmodal_2016]. A concrete example of this is the way we process emotional content in written words. @ponz_emotion_2014 recorded surface and intracranial electroencephalography (EEG) while people were reading disgusting and neutral words. They showed that the emotion effect occurred early in the EEG signals (after only 200ms post-stimulus onset) and was localized in brain areas, which are known to process disgust in a nonlinguistic context (i.e., insula). These results were replicated in a fMRI study, which further showed that repetitive transmagnetic stimulation (rTMS) over the insula specifically interfered with the disgust effect [@ziegler_words_2018]. Taken together, these results suggest that some aspects of word processing, such as those related to emotions, involve neural networks that are rather phylogenetically ancient, multimodal and not specifically dedicated to language processes. Such findings resonate with theories of grounded cognition [@barsalou_grounded_2008], which highlight the role of distributed (non-modular) neural simulation of sensory-motor-affective experience in the representation of knowledge [@barsalou_cognitive_2008]. We suggest that embodied theories of cognition provide a relevant and promising framework for the study of language and offer an interesting alternative to classical theories, particularly because embodied cognition theories propose to explain the development of cognitive functions in the brain from a phylogenetic and ontogenetic view.

Indeed, we believe that the literacy effects described by Kolinsky and Morais could well be explained in the context of the neural reuse model and massive redeployment hypothesis proposed by @anderson_evolution_2007, @anderson_grounds_2008, and @anderson_neural_2010. The model suggests that recent cognitive functions, phylogenetically speaking, make use of networks that support older functions such as vision and motor control [@anderson_grounds_2008]. More precisely, cognition would be achieved by using and reconfiguring existing networks to support new functions (without losing the previous ones), but also by the joint and temporary use of several neural networks to perform a task. Anderson conducted a serie of meta-analyses whose results support the *massive redeployment hypothesis*. In one of them, he showed that there is a negative correlation between the position of a brain region on the Y axis (postero-anterior) and the number of tasks this region is involved in [@anderson_circuit_2008; @anderson_grounds_2008], suggesting that older (posterior) parts of the brain are involved in more functions than more recently developed (mostly frontal) brain regions. In another study, Anderson showed that the more recent a cognitive function is, the more widely it is distributed over the brain: the most distributed cognitive functions are, in descending order, language, reasoning, memory, emotion, mental imagery, visual perception, action and attention (Anderson, 2008). Finally, in a meta-analysis of more than 1500 fMRI studies, Anderson and colleagues showed that the same brain areas are engaged in different tasks involving different cognitive processes [e.g., language, perception, action, memory, emotion, attention, @anderson_neural_2010; @anderson_quantifying_2011].

In terms of re-use, what applies to evolution supposedly applies to child development as well. Although oral language has appeared more than 350,000 years ago [@perreault_dating_2012], written language is a cultural invention that developed only recently [about 5000 years ago, @powell_writing_2009]. When children come to the task of learning to read, their brains have already acquired oral language. Thus, learning to read and write should make use of the same networks as those primarily used in oral language, object recognition (for the visual aspects of reading), and motor coordination (for the sensory-motor aspects of writing). For example, data showed a common neural activation for doing actions and for reading or hearing the corresponding action words [e.g., @hauk_neurophysiological_2004; @pulvermuller_functional_2005]. Similarly, @martin_neural_1996 showed that naming animals involved primary visual cortex, whereas naming tools involved premotor areas. Following Hebb’s proposal (1949) that frequently and simultaneously activated sets of cells form a functional unit (i.e., a network) and according to the principles of embodied cognition and neural reuse, oral and written language should use common networks that, in the process of learning to read and write, would become deeply intertwined with each other. This framework does not only explain why learning to read deeply influences brain networks but also why oral word processing is influenced by the spelling of the word [@ziegler_orthography_1998] and why hearing words recruits brain areas that are specialized in reading, such as the visual word form area [VWFA, @cohen_visual_2000]. Thus, embodied and grounded theories of cognition not only explain how literate language might re-use a subset of existing neural networks, but also why the acquisition of a semantic system for spoken words is done through our sensorimotor and affective experiences with the outside world, which, as a consequence implies extensive re-use of cognitive processes and brain regions outside the classic language and perisylvian system.

Interestingly, the same principles of interpenetrability between cognitive domains predict that written language should affect cognitive abilities that are not related to language, such as the perception of time. Time is a mental construction that we cannot experience directly through our senses. According to @boroditsky_how_2011-1 and @boroditsky_does_2001, language influences the way we think about time. As a matter of fact, several studies have shown that the passing of time depends on our reading direction: Mandarin speakers seem to have a representation of time that goes from right to left whereas English speakers seem to have a representation of time from left to right [e.g., @boroditsky_does_2001; @boroditsky_how_2011-1].

In summary, we believe it is important to move beyond a modular and unimodal perspective towards a multimodal approach that integrates developmental and evolutionary considerations. Many highly relevant models are already available in the literature, such as Anderson’s neural re-use model or the crossmodal learning (CML) model, which proposes to adopt a multi-modal perspective of cognition [@heinrich_crossmodal_2016]. Indeed, a truly crossmodal and multi-domain perspective on language and cognition that takes into account the perceiving and acting body is bound to yield rich rewards.
